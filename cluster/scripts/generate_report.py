#!/usr/bin/env python3
"""Generate PDF report of a cluster run.

As input it expects the path to a results directory containing the files generated
during the cluster run.
"""
from __future__ import annotations

import argparse
import contextlib
import enum
import logging
import pathlib
import pickle
import sys
import typing

import colorama
import pandas as pd

from cluster import report
from cluster.constants import (
    FULL_DF_FILE,
    REPORT_DATA_FILE,
    STATUS_PICKLE_FILE,
    SUBMISSION_HOOK_STATS_FILE,
)
from cluster.optimizers import Optimizer

LOGGER_NAME = "generate_report"


class ClusterRunType(enum.Enum):
    """Enumeration of possible cluster run types."""

    GRID_SEARCH = 0
    HP_OPTIMIZATION = 1


def initialize_logger(name: str, verbose: bool) -> logging.Logger:
    """Initialise and return logger instance."""

    class ColouredFormatter(logging.Formatter):
        STYLES = {
            "WARNING": colorama.Fore.YELLOW,
            "INFO": colorama.Fore.BLUE,
            "DEBUG": colorama.Fore.GREEN,
            "CRITICAL": colorama.Fore.RED + colorama.Style.BRIGHT,
            "ERROR": colorama.Fore.RED,
        }

        def __init__(self, *, fmt):
            logging.Formatter.__init__(self, fmt=fmt)

        def format(self, record: logging.LogRecord) -> str:  # noqa: A003
            msg = super().format(record)
            try:
                return f"{self.STYLES[record.levelname]}{msg}{colorama.Style.RESET_ALL}"
            except KeyError:
                return msg

    log_handler = logging.StreamHandler()
    log_handler.setFormatter(
        ColouredFormatter(fmt="[%(asctime)s] [%(name)s | %(levelname)s] %(message)s")
    )

    logger = logging.getLogger(name)
    logger.addHandler(log_handler)
    logger.setLevel(logging.DEBUG if verbose else logging.INFO)

    return logger


def detect_cluster_run_type(results_dir: pathlib.Path) -> ClusterRunType:
    """Try to detect the type of cluster run based on files in results_dir.

    Args:
        results_dir: Directory containing the files generated by the cluster run.

    Returns:
        Type of cluster run that was detected.

    Raises:
        RuntimeError: if none of the expected cluster run types could be determined.
    """
    # check for specific files that are only generated by the respective run type
    if (results_dir / STATUS_PICKLE_FILE).exists():
        return ClusterRunType.HP_OPTIMIZATION
    if (results_dir / REPORT_DATA_FILE).exists():
        return ClusterRunType.GRID_SEARCH

    raise RuntimeError(
        f"{results_dir} does not contain a supported cluster run.  Expected either a"
        f" file {STATUS_PICKLE_FILE} (for hp_optimization) or {REPORT_DATA_FILE} (for"
        " grid_search)."
    )


def load_submission_hook_stats(results_dir: pathlib.Path) -> dict[str, typing.Any]:
    """Load submission hook stats from file.

    Tries to load submission hook stats from the corresponding file in the results
    directory.
    If loading fails (e.g. because the file does not exist) an empty dictionary is
    returned.

    Args:
        results_dir:  Directory containing the file created by cluster utils.

    Returns:
        The loaded submission hook stats or an empty dictionary if loading fails.
    """
    logger = logging.getLogger(LOGGER_NAME)

    submission_hook_stats_file = results_dir / SUBMISSION_HOOK_STATS_FILE
    try:
        logger.debug("Read file %s", submission_hook_stats_file)
        with open(submission_hook_stats_file, "rb") as f:
            submission_hook_stats = pickle.load(f)

        if not isinstance(submission_hook_stats, dict):
            raise TypeError(
                f"Expected dictionary but got {type(submission_hook_stats)}."
            )
    except Exception as e:
        logger.error(
            "Failed to load submission hook stats from '%s': %s",
            submission_hook_stats_file,
            e,
        )
        submission_hook_stats = {}

    return submission_hook_stats


def generate_hp_optimization_report(
    results_dir: pathlib.Path, output_file: pathlib.Path
) -> None:
    logger = logging.getLogger(LOGGER_NAME)

    status_file = results_dir / STATUS_PICKLE_FILE
    logger.debug("Read file %s", status_file)
    with open(status_file, "rb") as f:
        optimizer: Optimizer = pickle.load(f)

    submission_hook_stats = load_submission_hook_stats(results_dir)

    if not isinstance(optimizer, Optimizer):
        logger.warning("Object loaded from '%s' is not of type Optimizer", status_file)

    report.produce_optimization_report(
        optimizer, output_file, submission_hook_stats, results_dir
    )


def generate_grid_search_report(
    results_dir: pathlib.Path, output_file: pathlib.Path
) -> None:
    logger = logging.getLogger(LOGGER_NAME)

    data_file = results_dir / FULL_DF_FILE
    other_info_file = results_dir / REPORT_DATA_FILE

    logger.debug("Read file %s", data_file)
    data = pd.read_csv(data_file)

    logger.debug("Read file %s", other_info_file)
    with open(other_info_file, "rb") as f:
        other_info = pickle.load(f)

    logger.debug("Produce grid search report")
    report.produce_gridsearch_report(
        data,
        output_file=output_file,
        **other_info,
    )


def main() -> int:
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "results_dir",
        type=pathlib.Path,
        help="Directory containing the generated files.",
        metavar="RESULTS_DIRECTORY",
    )
    parser.add_argument(
        "output",
        type=pathlib.Path,
        help="Where to save the report.",
        metavar="OUTPUT_FILE",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Overwrite existing output file without asking.",
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Enable verbose output."
    )
    args = parser.parse_args()

    logger = initialize_logger(LOGGER_NAME, args.verbose)

    if not args.results_dir.is_dir():
        logger.fatal("'%s' does not exist or is not a directory", args.results_dir)
        return 1

    try:
        run_type = detect_cluster_run_type(args.results_dir)
    except RuntimeError as e:
        logger.fatal("%s", e)
        return 1

    # do not overwrite existing files without confirmation
    if args.output.exists():
        if args.force:
            logger.warning(
                f"{args.output} already exists but will be overwritten due to --force."
            )
        else:
            val = input(f"{args.output} already exists.  Overwrite it? [y/N]: ")
            if val.lower() not in ["y", "yes"]:
                print("Do not overwrite existing file.  Abort.")
                return 1

    if run_type == ClusterRunType.HP_OPTIMIZATION:
        generate_hp_optimization_report(args.results_dir, args.output)
    elif run_type == ClusterRunType.GRID_SEARCH:
        generate_grid_search_report(args.results_dir, args.output)
    else:
        # this should never happen but catch it just in case...
        raise NotImplementedError("Unsupported cluster run type")

    logger.info("Saved report to %s", args.output)

    return 0


if __name__ == "__main__":
    with contextlib.suppress(KeyboardInterrupt):
        sys.exit(main())
